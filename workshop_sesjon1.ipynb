{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "workshop_sesjon1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/svvsaga/datascience_workshop/blob/peder/workshop_sesjon1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xS9TeayuGQ-"
      },
      "source": [
        "# Sesjon 1: \"Extract and Load\" med GCS og BigQuery\n",
        "\n",
        "I denne sesjonen skal vi lære grunnleggende operasjoner for å få data inn i GCS og BigQuery. Vi ser også på hvordan vi kan hente data fra et API, og dytte disse inn i BigQuery."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZxx7H7Iu8DX"
      },
      "source": [
        "# 1) Laste opp fil til GCS \n",
        "Det finnes flere måter å laster data inn til google cloud storage med ulike nivåer av kode-bruk:\n",
        "- gjennom GCP cloud console (grafisk grensesnitt i nettleser)\n",
        "- gjennom google cloud shell (terminal i nettleser)\n",
        "- gjennom lokal installasjon av google cloud SDK, som brukes gjennom terminalen\n",
        "- ved bruk av klientbliblioteker (f.eks. python client).\n",
        "\n",
        "I stegene under kan du selv velge hvilken fremgangsmåte som passer best for deg. Vi forklarer både hvordan du utfører stegene i web-grensesnittet og via kommandolinja.\n",
        "\n",
        "Dersom du ønsker å bruke kommandolinja, har vi under skrevet opp stegene for å klargjøre kommandolinja for bruk med GCP.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-AVdy_mOYC0"
      },
      "source": [
        "## Autentisering for bruk i python-kode\n",
        "\n",
        "Vi skal senere kjøre python-kode som krever at du er autentisert mot GCP. Kjør kode-cellen under for å bli autentisert (trykk på play-knappen)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fn0pLhpOdxJ"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "print('Authenticated') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_0xAVCLRcZf"
      },
      "source": [
        "Mens vi er i gang med oppsett kan vi like gjerne også definere prosjekt-IDen din i koden med en gang. Finn fram det fulle prosjektnavnet på prosjektet du har fått utdelt, og lim det inn under."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cokpZ11Rn4Q"
      },
      "source": [
        "project_id = \"<prosjekt-navn i GCP>\" # Endre denne\n",
        "dataset_id = \"workshop\"              # Ikke endre denne"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J31GmKgfY6Jt"
      },
      "source": [
        "## Oppsett av kommandolinja (valgfri)\n",
        "\n",
        "Dersom du ønsker å utføre stegene i denne notebooken via kommandolinja, utfør en av oppsettene under. Du trenger ikke å utføre begge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7lhUGJTtt8M"
      },
      "source": [
        "### Oppsett Cloud shell\n",
        "- Logg deg inn på GCP UI gjennom nettleseren din: https://console.cloud.google.com/ \n",
        "- Verifiser at prosjektet til workshop et valgt som arbeidsprosjekt\n",
        "- Aktiver cloud shell (\"Activate cloud shell\" knapp i øverste høyre hjørnet i UI), vent til cloud shell er provisjonert"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vL-wiUYpQwvd"
      },
      "source": [
        "### Oppsett Google cloud SDK\n",
        "- Last ned og installer Google cloud SDK for ditt system (i forkant av workshoppen): https://cloud.google.com/sdk/docs/install \n",
        "- initialiser cloud shell (verifiser at prosjektet er satt til ditt arbeids/workshop prosjekt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UQ9Sp0zIHRP"
      },
      "source": [
        "## 1.1) Last ned eksempelfiler fra GCS\n",
        "- Last ned 2 filer fra\n",
        " `gs://saga-trafikkdata-prod-pz8l_timetrafikkdata-ingest`. Dette er en bøtte som finnes i prosjektet `saga-trafikkdata-prod-pz8l` og som dere har fått rettigheter til å laste ned filer fra. Naviger til denne bøtta gjennom GCP web-grensesnittet. \n",
        "- Last ned de to filene:\n",
        " - `2021-03-23T09:31:31.945Z_2210582261344715.ndjson`\n",
        " - `2021-03-23T09:31:42.657Z_2210582990408255.ndjson`\n",
        "- Dette kan også oppnås ved å bruke følgende cloud/SDK shell kommando:\n",
        "\n",
        "```\n",
        "gsutil cp gs://bucket-name path/to/file.csv\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJQgYDZPdAET"
      },
      "source": [
        "## 1.2) Lag en GCS bøtte\n",
        "Lag en GCS bøtte der filene skal lastest opp. \n",
        "Dette kan gjøres enten gjennom grafisk user interface eller gjennom cloud shell/SDK\n",
        "\n",
        "- I grafisk user interface naviger til Cloud Storage -> Browser og trykk på \"Create bucket\" \n",
        "- Velg et (globalt unik!) navn til bucket og konfigurer lagringsopsjoner\n",
        "  - standard storage\n",
        "  - multi-region, eu\n",
        "  - uniform access\n",
        "\n",
        "Det samme kan oppnås ved bruk av følgende cloud/SDK shell kommando:\n",
        "\n",
        "```\n",
        "gsutil mb -b -c standard -l EU gs://bucket-name\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYcemB6tdAG9"
      },
      "source": [
        "## 1.3) Laste opp filer til GCS bucket\n",
        "\n",
        "Vi skal nå laste opp filene som vi lastet ned tidligere. Dette kan vi også gjøre via web-grensesnittet, eller via kommandolinja (f.eks. i cloud shell). Du velger selv fremgangsmåte.\n",
        "\n",
        "Dersom du vil utføre dette via kommandolinja:\n",
        "\n",
        "```\n",
        "gsutil cp path/to/file.ndjson gs://bucket-name\n",
        "```\n",
        "\n",
        "Det er mulig å laste opp enkelte filter eller en liste med filter samtidig\n",
        "\n",
        "```\n",
        "gsutil cp path/to/*.ndjson gs://bucket-name\n",
        "```\n",
        "\n",
        "Til slutt verifiser at filene har blitt lastet opp, enten ved grafisk user interace (storage browser) eller gjennom cloud shell.\n",
        "\n",
        "```\n",
        "gsutil ls gs://bucket-name\n",
        "```\n",
        "\n",
        "En oversikt over gsutil kommandos finnes her: https://cloud.google.com/storage/docs/gsutil eller ved bruk av `gsutil help` i cloud shell terminal.\n",
        "\n",
        "### Tips: Opplasting av store datamengder til GCS\n",
        "\n",
        "På Saga-prosjektet har vi opplevd at det å bruke gsutil fungerer greit til middels store opplastinger til GCS, men når vi skulle laste opp flere terabyte med data til GCS ble det rett og slett for treigt. Da var det verktøyet [rclone som ble løsningen for oss](https://rclone.org/googlecloudstorage/). Vi anbefaler derfor rclone dersom du oppdager at gsutil bruker lang tid på opplastingen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mmCYn_KpPVr"
      },
      "source": [
        "# 2) Importere data direkte fra GCS til BigQuery\n",
        "Etter dataene har blitt lastet opp til GCS skal de ofte importeres til BigQuery til videre analyse. BigQuery er et datavarehus som kan lagre store datasett hvor man kan bruke SQL spørringer for å hente ut data.\n",
        "\n",
        "BigQuery støtter en del formater direkte: \n",
        "- Avro\n",
        "- CSV\n",
        "- JSON\n",
        "- ORC\n",
        "- Parquet\n",
        "\n",
        "Dataene kan importeres fra GCS bøtter, lokale filer (max 10MB upload per fil), google drive, BigTable eller genereres on-the-fly (lage en tom tabell).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIfAvWpNwRyx"
      },
      "source": [
        "## 2.1) Opprette et BigQuery-datasett\n",
        "\n",
        "Her vil vi importere dataene vi har lastet opp til en GCS bøtte inn i BigQuery. BigQuery organiserer dataene i prosjekter, datasett og så tabeller. Før vi kan laste opp dataene i en tabell må vi derfor lage et nytt datasett.\n",
        "\n",
        "**Det nye datasettet skal ha navnet workshop.**\n",
        "\n",
        "Via web-grensesnittet kan dette gjøres ved å først navigere deg inn i BigQuery, velge ditt prosjekt og så klikke på \"Create dataset\". Location skal være den samme som for bøtta vi lagde, altså multi-region, EU.\n",
        "\n",
        "Dersom du vil bruke kommandolinja, bruk følgende kommando.\n",
        "\n",
        "```\n",
        "bq --location=EU mk -d dataset_name\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CqV799GwP7e"
      },
      "source": [
        "## 2.2) Importere data inn i ny tabell\n",
        "\n",
        "Når datasett har blitt opprettet kan vi importere dataene fra GCS inn til en BigQuery tabell: `dataset_name.table_name`, hvor \"dataset_name\" altså skal være workshop.\n",
        "\n",
        "I web-grensesnittet, velg datasettet \"workshop\" og trykk på \"Create table\".\n",
        "Velg at filene skal lastes opp fra GCS. Gå via \"Browse\"-knappen for å finne bøtta og filene du vil laste opp. \"Browse\"-dialogen får det til å virke som at man må velge en fil, men det er i stedet mulig å skrive inn `*` i \"name\"-feltet for å matche alle filer, eller `*.ndjson` for å matche alle filer som slutter på `.ndjson`. Gjør dette slik at du får lastet opp begge filene.\n",
        "\n",
        "Deretter kan du velge et tabellnavn (f.eks. \"timestrafikkdata\"). I tillegg må du huke av for \"auto detect\" for skjemaet. Deretter kan du velge \"Create table\".\n",
        "\n",
        "Dersom du heller vil bruke kommandolinja:\n",
        "\n",
        "```\n",
        "bq --location=eu load --autodetect --source_format=NEWLINE_DELIMITED_JSON <DATASET>.<TABLE_ID> gs://<bucket_name>/2021-03-23T09:31:31.945Z_2210582261344715.ndjson\n",
        "```\n",
        "\n",
        "Det er også mulig å laste opp alle filer samtidig ved bruk av `*`:\n",
        "\n",
        "```\n",
        "bq --location=eu load --autodetect --source_format=NEWLINE_DELIMITED_JSON <DATASET>.<TABLE_ID> gs://<bucket_name>/*.ndjson\n",
        "```\n",
        "\n",
        "`--autodetect` er en flag som la BigQuery velge selv hvilken data type de ulike felter sannsynligvis har. Dette gjør ofte en rimelig bra jobb i første omgang og er veldig nyttig når man laster inn filer i JSON format som kan ha en kompleks, nestet struktur.\n",
        "\n",
        "Alternativt er det mulig å angi en custom schema etter filnavn. Her kan det brukes en JSON-fil som definerer schema, eller en komma-separarert string av format `FELT_NAVN:DATA_TYPE,...`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moEKqQ9_wR1X"
      },
      "source": [
        "## 2.3) Verifser import med en SQL spørring\n",
        "\n",
        "Gjennom web-grensesnittet skal du nå kunne se en ny tabell. I alle fall dersom du oppdaterer siden. Verifiser at denne inneholder data ved å opprette og kjøre følgende SQL-spørring. Du må selv angi `<table_id>`.\n",
        "\n",
        "```\n",
        "SELECT *\n",
        "FROM `workshop.<table_id>`\n",
        "```\n",
        "\n",
        "I denne seksjonen gjør vi en vri. Vi viser ikke hvordan du utfører en BigQuery-spørring via kommandolinja (men du kan gjøre det om du vil). I stedet viser vi hvordan du gjør det rett fra denne notebooken, både via å kjøre python-kode, og via ipython \"inline magic\".\n",
        "\n",
        "Før vi kan hente ut data fra BigQuery via SQL spørringer så må vi definere prosjekt, dataset og tabel IDene."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HWGHhyEDqfp"
      },
      "source": [
        "from google.cloud import bigquery\n",
        "\n",
        "table_id = \"<table id>\"              # Endre denne til navnet på tabellen din"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS3mZKqjfElZ"
      },
      "source": [
        "Her benytter vi oss av BigQuery \"inline magic\" kommandoer for å kjøre en SQL spørring direkte fra notebooken. Uheldigvis støtter ikke denne metoden å bytte ut \"dataset_id\" og \"table_id\" automatisk, så disse må du lime inn selv.Inline magic kommandoer kjøres ved bruk av `%%` øverst i notebook cellen. I tillegg angir vi et BigQuery prosjekt og en lokal python variable dataene skal lagres in."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eap6zUDX8Fax"
      },
      "source": [
        "%%bigquery --project $project_id trip_df\n",
        "SELECT *\n",
        "FROM `workshop.<table_id>`"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqhAVecCfP2x"
      },
      "source": [
        "Her gjør vi det samme som over, men nå benytter vi oss av BigQuery klientbiblioteket for å kjøre den samme SQL spørringen:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "527V0GV3OVo_"
      },
      "source": [
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "trip_df = client.query(\n",
        "    \"\"\"\n",
        "    SELECT *\n",
        "    FROM `%s.%s`\n",
        "    \"\"\" % (dataset_id, table_id)\n",
        ").to_dataframe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyahU54aNJyf"
      },
      "source": [
        "trip_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYbnJdG_p8gz"
      },
      "source": [
        "## 2.4) Automatisk import fra GCS til BigQuery er ganske rå!\n",
        "\n",
        "I skrivende stund har vi 370 GB++ med timesaggregert trafikkdata i bøtta `gs://saga-trafikkdata-prod-pz8l_timetrafikkdata-ingest` som vi så på tidligere. I denne oppgaven skal vi vise at det er null problem å laste opp hele dette datasettet til BigQuery, og det tar ikke engang særlig lang tid.\n",
        "\n",
        "I web-grensesnittet, naviger deg tilbake til workshop-datasettet, og velg \"Create table\". Igjen skal datakilden være GCS. \"Browse\"-knappen lar deg ikke velge bøtter som ligger i andre prosjekter enn det du jobber i nå. Derfor må du denne gangen selv fylle inn `gs://saga-trafikkdata-prod-pz8l_timetrafikkdata-ingest/*.ndjson`. Ellers skal vi gjøre det samme som sist gang. Velg deg et tabellnavn og huk av for \"auto detect\" på skjema. Trykk så \"Create table\". Dette vil antakelig ta rundt 3-4 minutter. Du kan følge med på importjobbens status ved å navigere deg inn på \"job history\".\n",
        "\n",
        "For å gjøre dette med kommandolinja kan vi gjøre akkurat det samme som sist gang, foruten at kildebøtta er ulik. Kommandoen blir altså noe slikt som:\n",
        "\n",
        "```\n",
        "bq --location=eu load --autodetect --source_format=NEWLINE_DELIMITED_JSON <DATASET>.<TABLE_ID> gs://<bucket_name>/*.ndjson\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tG8SpmlwlHDb"
      },
      "source": [
        "# 3) Importer data fra API\n",
        "\n",
        "Av og til er det data fra et API man ønsker å utforske i BigQuery. Her viser vi hvordan man kan gjøre dette. Denne gangen skal vi ta en titt på [data fra trafikkdata-APIet](https://www.vegvesen.no/trafikkdata/api/?query=%7B%0A%20%20trafficRegistrationPoints%20%7B%0A%20%20%20%20id%0A%20%20%20%20name%0A%20%20%20%20location%20%7B%0A%20%20%20%20%20%20coordinates%20%7B%0A%20%20%20%20%20%20%20%20latLon%20%7B%0A%20%20%20%20%20%20%20%20%20%20lat%0A%20%20%20%20%20%20%20%20%20%20lon%0A%20%20%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20%7D%0A%20%20%20%20%7D%0A%20%20%7D%0A%7D%0A). Trykk på \"play\"-knappen for å hente dataene i nettleseren. Her ser vi altså at dataene består av id, navn og posisjon for landets trafikkregistreringspunkter. Disse dataene skal vi bruke senere for å berike timestrafikkdataene med posisjon.\n",
        "\n",
        "I denne seksjonen bruker vi python som kan kjøres rett i notebooken.\n",
        "\n",
        "Først skal vi installere en \"pre-release\"-versjon av pakken gql, som lar oss gjøre spørringer mot API-et via GraphQL."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzhqG5WblHQZ"
      },
      "source": [
        "!pip install --pre gql[all]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHq6ytoRPnvF"
      },
      "source": [
        "Så utfører vi det faktiske API-kallet, og printer det første trafikkregistreringspunktet i responsen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWHuAcG9nFEv"
      },
      "source": [
        "import json\n",
        "from gql import gql, Client\n",
        "from gql.transport.aiohttp import AIOHTTPTransport\n",
        "from gql.transport.requests import RequestsHTTPTransport\n",
        "\n",
        "# Setup connection\n",
        "transport = RequestsHTTPTransport(url=\"https://www.vegvesen.no/trafikkdata/api/\", verify=True, retries=3)\n",
        "\n",
        "# Create a GraphQL client using the defined connection\n",
        "gql_client = Client(transport=transport, fetch_schema_from_transport=True)\n",
        "\n",
        "# Provide a GraphQL query\n",
        "query = gql(\n",
        "    \"\"\"\n",
        "    {\n",
        "      trafficRegistrationPoints {\n",
        "        id\n",
        "        name\n",
        "        location {\n",
        "          coordinates {\n",
        "            latLon {\n",
        "              lat\n",
        "              lon\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# Execute the query. The result is a native python dictionary.\n",
        "data = gql_client.execute(query)\n",
        "trafficRegistrationPoints = data[\"trafficRegistrationPoints\"]\n",
        "\n",
        "# Print the first traffic registration point in the list\n",
        "print(json.dumps(trafficRegistrationPoints[0], indent=4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lshq9h0OR7Ag"
      },
      "source": [
        "Før vi laster opp dataene til BigQuery må vi lage den fulle identifikatoren til den nye tabellen vi skal opprette."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Y2j0sgPPCj0"
      },
      "source": [
        "table_id = \"trafikkregistreringspunkter\"\n",
        "\n",
        "table_path = \".\".join([project_id, dataset_id, table_id])\n",
        "table_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wum-t3N6Sy3o"
      },
      "source": [
        "Dermed er vi klar til å laste opp dataene til BigQuery."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSRNHo-FONj-"
      },
      "source": [
        "bq_client = bigquery.Client(project=project_id)\n",
        "load_job = bq_client.load_table_from_json(data[\"trafficRegistrationPoints\"], table_path)\n",
        "result = load_job.result()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gk3IoZU_TAtF"
      },
      "source": [
        "Du kan nå verifisere at dataene har kommet inn i BigQuery. Hvordan du vil gjøre dette er opp til deg."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qcap7sdZFul"
      },
      "source": [
        "## 3.1) Frivillige oppgaver dersom du har tid til overs\n",
        "\n",
        "**Oppgave 1:** Datastrukturen til `trafficRegistrationPoints` har unødvendig nøsting, i form av at lengde- og breddegrad ligger inn i \"location.coordinates.latLon\". Klarer du å flate ut strukturen i python-kode før dataene skrives til BigQuery? Vi ønsker altså å ende opp med at hvert element bare skal bestå av \"id\", \"name\", \"lat\" og \"lon\". **Hint:** bruk funksjonen `map(fun, iter)` til å mappe om hvert element i lista.\n",
        "\n",
        "**Oppgave 2:** Når vi laster opp data til BigQuery med `load_table_from_json(...)` vil BigQuery forsøke å gjette skjemaet, og alle felter vil bli nullable (i motsetning til required). Det er mulig å angi skjemaet selv. Dette gjør at vi kan sette alle feltene til required, som vi ønsker at de skal være. Gjør dette. **Hint:** Send inn et ekstra argument `job_config=..` til `load_table_from_json(...)`, og sett `schema` i `LoadJobConfig`-objektet du sender inn. [Dokumentasjonen for denne funksjonen kan leses her.](https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.client.Client.html#google.cloud.bigquery.client.Client.load_table_from_json)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AWD55diV55k"
      },
      "source": [
        "# Utdatert ekstramateriale"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJruzauHEsJY"
      },
      "source": [
        "## Importere data med Dataflow inn i BigQuery\n",
        "\n",
        "TODO: Tekst her\n",
        "\n",
        "Først må vi installere Apache Beam i notebooken. Husk å trykke \"Restart runtime\" etter å ha installert python-modulene. Dette er nødvendig for at notebooken skal kunne bruke nyinstallerte moduler."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPxEdqNz99v4"
      },
      "source": [
        "!pip install 'apache-beam[interactive]' # For running beam interactively\n",
        "!pip install 'apache-beam[gcp]'         # For running beam on GCP Dataflow\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6oJvw2nk1lR"
      },
      "source": [
        "# TODO: Gjere dette i starten\n",
        "%env PROJECT_ID=saga-workshop-dtest-9hsr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0FOfn4riQHa"
      },
      "source": [
        "Vi må også installere Apache Beam-pakker tilpasset bruk med GCP. Igjen, husk å trykke \"Restart runtime\" etterpå."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJFDux5pJs7P"
      },
      "source": [
        "# Ta vekk\n",
        "!gcloud auth login"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zx7pxhYMQzot"
      },
      "source": [
        "# Ta vekk\n",
        "!gcloud projects list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUHxJlZwHT3P"
      },
      "source": [
        "!gcloud config set project $PROJECT_ID"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umjXdOw7EQ6t"
      },
      "source": [
        "!gcloud auth application-default login"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyHn7nYKvZh0"
      },
      "source": [
        "import csv\n",
        "import os\n",
        "\n",
        "# Test\n",
        "\n",
        "import apache_beam as beam\n",
        "import apache_beam.runners.interactive.interactive_beam as ib\n",
        "import apache_beam.io.fileio\n",
        "from apache_beam.runners.interactive.interactive_runner import InteractiveRunner\n",
        "from apache_beam.options import pipeline_options\n",
        "from apache_beam.options.pipeline_options import GoogleCloudOptions\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OnBkheR91AG"
      },
      "source": [
        "options = pipeline_options.PipelineOptions()\n",
        "\n",
        "project_id = os.environ['PROJECT_ID']\n",
        "\n",
        "# Set the pipeline mode to stream the data from Pub/Sub.\n",
        "options.view_as(pipeline_options.StandardOptions).streaming = False\n",
        "options.view_as(GoogleCloudOptions).project = project_id\n",
        "\n",
        "p = beam.Pipeline(InteractiveRunner(), options=options)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5yiF2ZLF0ZT"
      },
      "source": [
        "Dere skal nå få lov å sette sammen noen enkle Beam-pipelines selv. Under har vi inkludert en liste med 8 transformasjoner som utgjør deres \"byggesett\". Dere må selv velge ut de nødvendige transformasjonene basert på hva oppgaven spør etter.\n",
        "\n",
        "- Finne filer som matcher et gitt mønster: `beam.io.fileio.MatchFiles(pattern))`\n",
        "- Filtrere vekk elementer basert på en betingelse: `beam.Filter(condition_check_function)`\n",
        "- Hente ut N tilfeldige elementer fra en større samling (PCollection): `beam.combiners.Sample.FixedSizeGlobally(N)`\n",
        "- Gjøre om filstier til faktisk lesbare filer: `beam.io.fileio.ReadMatches()`\n",
        "- Utføre en egendefinert transformasjon på hvert element. Passer best når output av transformasjonen er et enkelt element: `beam.Map(transform_function)`\n",
        "- Utføre en egendefinert transformasjon på hvert element, hvor transformasjonen skal resultere i en liste med output-elementer. Denne vil da slå sammen alle output-elementer fra hver transformasjon i en felles liste: `beam.FlatMap(transform_function)`\n",
        "\n",
        "En fullstendig liste med innebygde transformasjoner kan sees her: https://beam.apache.org/documentation/transforms/python/overview/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjvInG2GAmbh"
      },
      "source": [
        "file_pattern = \"gs://saga-trafikkdata-prod-pz8l_timetrafikkdata-ingest/2021-04-10*\" # April 10th, in the first 10 minutes of the 10th hour\n",
        "\n",
        "file_paths = (p\n",
        "  | \"find files\" >> beam.io.fileio.MatchFiles(file_pattern))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8F-qttfF6cAd"
      },
      "source": [
        "Vi har nå definert en minimal pipeline som finner filstier på alle filer i GCS som matcher mønsteret i `file_pattern`. For å sjekke at dette fungerer som det skal, kan vi bruke `ib.show`, som vi får fra den interaktive modulen til Apache Beam.\n",
        "\n",
        "Men vi har ikke lyst å liste ut absolutt alle filene som matcher mønsteret. La oss heller plukke ut 10 tilfeldige filstier. Dette gjør vi med den innebygde \"sample\"-transformasjonen `beam.combiners.Sample.FixedSizeGlobally(n)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Lmtb144BIbV"
      },
      "source": [
        "file_samples = file_paths | \"sample 10 files\" >> beam.combiners.Sample.FixedSizeGlobally(10)\n",
        "ib.show(file_samples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyWDDrIg7V-O"
      },
      "source": [
        "Når vi har bekreftet at vi ser filstiene kan vi fortsette med pipelinen vår. I koden under har vi lagt til tre ekstra steg:\n",
        "\n",
        "1. Et steg som klargjør filene til lesing\n",
        "2. Et steg som transformerer innholdet i filene til en liste av python dictionaries\n",
        "3. Et steg som skriver python dictionaries til BigQuery\n",
        "\n",
        "Funksjonen `transform_file_to_objects` inneholder innmaten i steg 2. Slik den er nå vil den bare parse hver rad i filene til en python dictionary som gjenspeiler json-strukturen. Dette betyr at foreløpig vil pipelinen resultere i den samme datamodellen/skjema i BigQuery som vi fikk når vi lastet opp json-filene direkte.\n",
        "\n",
        "Vi ønsker som sagt å også få med \"ingest time\", altså tidspunktet for vi mottok filene. Dette tidspunktet er en del av filnavnet. Dette ser vi dersom vi inspiserer filstiene vi fikk i steget over.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qFi7AYei5QP"
      },
      "source": [
        "import json\n",
        "\n",
        "# This function transforms each line of an .ndjson file into a python dictionary.\n",
        "# A python dictionary is a record type with keys and values, and is therefore a good fit for json data\n",
        "# Later, we will send our dictionaries to Apache Beam's BigQuery library\n",
        "def transform_file_to_objects(file):\n",
        "  file_path = file.metadata.path\n",
        "  file_contents = file.read_utf8()\n",
        "  json_objects = []\n",
        "  lines = file_contents.split('\\n')\n",
        "  for line in lines:\n",
        "    if line and line.startswith(\"{\"):\n",
        "        # We use json.loads to parse each line (which contains json) into a python dictionary\n",
        "        timestrafikkdata_row = json.loads(line)\n",
        "        # timestamp_string = regex(...)\n",
        "        #\n",
        "        # TODO: Extract ingest time from the file_path parameter available in this function, and add it to timestrafikkdata_row\n",
        "        # \"Transform timestamp to date type and add it to timestrafikkdata_row\"\n",
        "        #\n",
        "        # timestamp = Date(timestamp_string)\n",
        "        # timestrafikkdata_row[\"ingest_time\"] = timestamp\n",
        "        json_objects.append(timestrafikkdata_row)\n",
        "  return json_objects\n",
        "\n",
        "# file.read_utf8()\n",
        "\n",
        "write_results = (file_paths\n",
        "  | \"prepare files for reading\" >> beam.io.fileio.ReadMatches()\n",
        "  | \"transform to objects\" >> beam.FlatMap(transform_file_to_objects)\n",
        "  | \"write to BigQuery\" >> beam.io.WriteToBigQuery(\"saga-workshop-dtest-9hsr:workshop.df5\",\n",
        "                                                   schema=beam.io.gcp.bigquery.SCHEMA_AUTODETECT,\n",
        "                                                   write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,\n",
        "                                                   create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
        "                                                   custom_gcs_temp_location=\"gs://saga-workshop-dtest-9hsr_dataflow_files/temp\",\n",
        "                                                   method=beam.io.gcp.bigquery.WriteToBigQuery.Method.FILE_LOADS\n",
        "                                                   ))\n",
        "\n",
        "# p.run().wait_until_finish()\n",
        "\n",
        "#  | \"extract contents\" >> beam.Map(lambda file: file.read_utf8()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-TDKGtlpRCv"
      },
      "source": [
        "from apache_beam.runners import DataflowRunner\n",
        "from apache_beam.options.pipeline_options import WorkerOptions\n",
        "\n",
        "df_options = pipeline_options.PipelineOptions()\n",
        "\n",
        "# Set the project to the default project in your current Google Cloud\n",
        "# environment.\n",
        "df_options.view_as(GoogleCloudOptions).project = project_id\n",
        "df_options.view_as(GoogleCloudOptions).region = 'europe-west4'\n",
        "df_options.view_as(GoogleCloudOptions).service_account_email = 'dataflow@{}.iam.gserviceaccount.com'.format(project_id)\n",
        "df_options.view_as(WorkerOptions).network = 'vpc-network'\n",
        "#df_options.view_as(GoogleCloudOptions).subnetwork = 'regions/europe-west4/subnetworks/vpc-network'\n",
        "\n",
        "# Storage for uploading Dataflow code and temporary files\n",
        "dataflow_gcs_location = 'gs://saga-workshop-dtest-9hsr_dataflow_files'\n",
        "df_options.view_as(GoogleCloudOptions).staging_location = '%s/staging' % dataflow_gcs_location\n",
        "df_options.view_as(GoogleCloudOptions).temp_location = '%s/temp' % dataflow_gcs_location\n",
        "\n",
        "runner = DataflowRunner()\n",
        "runner.run_pipeline(p, options=df_options)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSPrX2FQsyI2"
      },
      "source": [
        "## Laste opp filer med klientbiblioteker.\n",
        "- Filer kan også lastes opp ved bruk av client bibliotheker. En oversikt over støttede programmeringsspråk finnes her: https://cloud.google.com/storage/docs/reference/libraries \n",
        "\n",
        "- I cellen under finnes det et minimalt eksempel med python client bibliothek.\n",
        "\n",
        "- For å kjøre python kode direkte fra denne notebook, må google cloud brukeren først autentiseres. \n",
        "\n",
        "- Vi bruker google.colab.auth python bibliothek til autentisering og google.cloud.storage til interaksjon med storage buckets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iE9_fDMveND"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "print('Authenticated') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0Vxzeugtgln"
      },
      "source": [
        "from google.cloud import storage\n",
        "from google.cloud import bigquery\n",
        "%load_ext google.colab.data_table\n",
        "\n",
        "proj = # TODO define workshop-project\n",
        "bucket =  # TODO define gs://bucket-name\n",
        "file_name = # TODO define lokal file name\n",
        "blob_name = # TODO define GCS blob name for the file\n",
        "client = storage.Client(project = proj) # initialize client and set the billing project. NB: vi trenger ikke å gjenta credentials her.\n",
        "bucket_object = client.get_bucket(bucket) # define the target bucket\n",
        "blob = bucket.blob(blob_name) # make a file blob\n",
        "blob.upload_from_filename(file_name) # upload the content from lokal file to GCS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXz3Q-1nx7oM"
      },
      "source": [
        "NB: Det finnes klientbiblioteker til de fleste GCP verktøy/tjenester. F.eks i python: https://cloud.google.com/python/docs/reference "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqg-g-FPfEDQ"
      },
      "source": [
        "# Outline\n",
        "Her tar vi utgangspunkt i GPS-data fra Peder sin kjøretur fra Trondheim til Ørsta\n",
        "\n",
        "1. Laste opp fil til GCS via terminalen (Cloud Shell/gsutil)\n",
        "  - Nevne at det også kan gjøres via GUI og client libs\n",
        "  - Nevne rclone for større datamengder\n",
        "2. Importere data direkte fra GCS til BigQuery (Cloud Shell/bq)\n",
        "  - Nevne at det også kan gjøres via GUI og client libs\n",
        "3. Utføre en enkel BigQuery-spørring for å se at vi har fått inn data\n",
        "4. Legge til datamapping i en Dataflow-jobb, og importere og mappe data inn i BigQuery\n",
        "  - Kan bruke samme data som i pkt. 2, men at vi i DF-jobben forbedrer datamodellen, f.eks. ved å slå sammen to kolonner lat, long til en geography-kolonne\n",
        "  - Bør vise Dataflow-GUI mens jobben kjører\n",
        "5. Sjekke i BigQuery at dataene har komt inn på riktig format\n",
        "6. Vise at dei kan laste inn alle dataene frå GCS til BigQuery via GUI\n",
        "7. Seie litt om når Dataflow er riktig verktøy\n",
        "8. Meir? Skal vi joine med geolokasjon i denne bolken eller seinare?\n",
        "  - https://www.vegvesen.no/trafikkdata/api/?query=%7B%0A%20%20trafficRegistrationPoints%20%7B%0A%20%20%20%20id%0A%20%20%20%20name%0A%20%20%20%20location%20%7B%0A%20%20%20%20%20%20coordinates%20%7B%0A%20%20%20%20%20%20%20%20latLon%20%7B%0A%20%20%20%20%20%20%20%20%20%20lat%0A%20%20%20%20%20%20%20%20%20%20lon%0A%20%20%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20%7D%0A%20%20%20%20%7D%0A%20%20%7D%0A%7D%0A\n",
        "\n",
        "## Andre forslag:\n",
        "- Bruke python libs + dataframes + pandas til noe (sesjon 1 eller 2?)\n",
        "- Vurdere å vise Cloud Function som henter data og dytter det inn i Dataflow eller BigQuery?\n",
        "- Andre verktøy: AirByte, FME og DBT\n",
        "\n",
        "## Nytt forslag:\n",
        "- Ta utgangspunkt i trafikkdata (saga-trafikkdata-prod-pz8l_timetrafikkdata-ingest)\n",
        "- Gjere cirka det samme som før i steg 1, 2, 3\n",
        "- Då ser vi at dataene for det første er veldig nøsta og at vi ikkje veit \"ingest time\" som er ein del av filnamna.\n",
        "- Steg 4 (dataflow) vil då kunne gi verdi pga:\n",
        "  1. Kan ha med ingesttime inn i BigQuery\n",
        "  2. Kan hente ut berre totalt trafikkvolum, sidan vi ikkje bryr oss om resten"
      ]
    }
  ]
}