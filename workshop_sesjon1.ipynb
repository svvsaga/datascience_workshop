{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "workshop_sesjon1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/svvsaga/datascience_workshop/blob/martin/workshop_sesjon1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b68G62Ew84f"
      },
      "source": [
        "# Outline\n",
        "Her tar vi utgangspunkt i GPS-data fra Peder sin kjøretur fra Trondheim til Ørsta\n",
        "\n",
        "1. Laste opp fil til GCS via terminalen (Cloud Shell/gsutil)\n",
        "  - Nevne at det også kan gjøres via GUI og client libs\n",
        "2. Importere data direkte fra GCS til BigQuery (Cloud Shell/bq)\n",
        "  - Nevne at det også kan gjøres via GUI og client libs\n",
        "3. Utføre en enkel BigQuery-spørring for å se at vi har fått inn data\n",
        "4. Legge til datamapping i en Dataflow-jobb, og importere og mappe data inn i BigQuery\n",
        "  - Kan bruke samme data som i pkt. 2, men at vi i DF-jobben forbedrer datamodellen, f.eks. ved å slå sammen to kolonner lat, long til en geography-kolonne\n",
        "  - Bør vise Dataflow-GUI mens jobben kjører\n",
        "5. Sjekke i BigQuery at dataene har komt inn på riktig format\n",
        "6. Vise at dei kan laste inn alle dataene frå GCS til BigQuery via GUI\n",
        "7. Meir? Skal vi joine med geolokasjon i denne bolken eller seinare?\n",
        "  - https://www.vegvesen.no/trafikkdata/api/?query=%7B%0A%20%20trafficRegistrationPoints%20%7B%0A%20%20%20%20id%0A%20%20%20%20name%0A%20%20%20%20location%20%7B%0A%20%20%20%20%20%20coordinates%20%7B%0A%20%20%20%20%20%20%20%20latLon%20%7B%0A%20%20%20%20%20%20%20%20%20%20lat%0A%20%20%20%20%20%20%20%20%20%20lon%0A%20%20%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20%7D%0A%20%20%20%20%7D%0A%20%20%7D%0A%7D%0A\n",
        "\n",
        "## Andre forslag:\n",
        "- Bruke python libs + dataframes + pandas til noe (sesjon 1 eller 2?)\n",
        "- Vurdere å vise Cloud Function som henter data og dytter det inn i Dataflow eller BigQuery?\n",
        "- Andre verktøy: AirByte, FME og DBT\n",
        "\n",
        "## Nytt forslag:\n",
        "- Ta utgangspunkt i trafikkdata (saga-trafikkdata-prod-pz8l_timetrafikkdata-ingest)\n",
        "- Gjere cirka det samme som før i steg 1, 2, 3\n",
        "- Då ser vi at dataene for det første er veldig nøsta og at vi ikkje veit \"ingest time\" som er ein del av filnamna.\n",
        "- Steg 4 (dataflow) vil då kunne gi verdi pga:\n",
        "  1. Kan ha med ingesttime inn i BigQuery\n",
        "  2. Kan hente ut berre totalt trafikkvolum, sidan vi ikkje bryr oss om resten"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xS9TeayuGQ-"
      },
      "source": [
        "TODO: Forklare at vi skal autentisere oss\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZxx7H7Iu8DX"
      },
      "source": [
        "### 1. Laste opp fil til GCS \n",
        "Det finnes flere måter å laster data inn til google cloud storage med ulike nivåer av kode-bruk:\n",
        "- gjennom google cloud platform grafisk user interface (nettleser)\n",
        "- gjennom google cloud shell (nettleser)\n",
        "- gjennom lokal installasjon av google cloud SDK\n",
        "- ved bruk av client bliblioteker (f.eks. python client)\n",
        "\n",
        "Her vil vi introdusere muligheter ved bruk av cloud shell og lokal installasjon av cloud SDK. \n",
        "\n",
        "Kommandoene er i utgangspunkt de samme, men oppsett er litt forskjellig: \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J31GmKgfY6Jt"
      },
      "source": [
        "#### Oppsett Cloud shell\n",
        "- Logg deg inn på Google Cloud Platform UI gjennom nettleseren: https://console.cloud.google.com/ \n",
        "- Verifiser at prosjektet til workshop et valgt som arbeidsprosjekt\n",
        "- Aktiver cloud shell (\"Activate cloud shell\" knapp i øverste høyre hjørnet i UI), vent til cloud shell er provisjonert\n",
        "\n",
        "\n",
        "#### Oppsett Google cloud SDK\n",
        "- Last ned og installer Google cloud SDK for ditt system (i forkant av workshoppen): https://cloud.google.com/sdk/docs/install \n",
        "- initialiser cloud shell (verifiser at prosjektet er satt til ditt arbeids/workshop prosjekt)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UQ9Sp0zIHRP"
      },
      "source": [
        "#### Last ned eksempelfiler fra GCS (gjennom UI)\n",
        "- Last ned 2 filer fra\n",
        " `gs://saga-trafikkdata-prod-pz8l_timetrafikkdata-ingest`. Dette er en bøtte som finnes i prosjektet `saga-trafikkdata-prod-pz8l` og som dere har fått rettigheter til å laste ned filer fra.\n",
        "- Last ned de to filene:\n",
        " - `2021-03-23T09:31:31.945Z_2210582261344715.ndjson`\n",
        " - `2021-03-23T09:31:42.657Z_2210582990408255.ndjson`\n",
        "- Dette kan oppnås ved å bruke følgende cloud/SDK shell kommando:\n",
        "\n",
        "```\n",
        "gsutil cp gs://bucket-name path/to/file.csv\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJQgYDZPdAET"
      },
      "source": [
        "#### Lag en GCS bøtte\n",
        "Lag en GCS bøtte der filene skal lastest opp. \n",
        "Dette kan gjøres enten gjennom grafisk user interface eller gjennom cloud shell/SDK\n",
        "\n",
        "- I grafisk user interface naviger til Cloud Storage -> Browser og trykk på \"Create bucket\" \n",
        "- Velg et (globalt unik!) navn til bucket og konfigurer lagringsopsjoner\n",
        "  - standard storage\n",
        "  - single-region, europe-west3\n",
        "  - uniform access\n",
        "\n",
        "Det samme kan oppnås ved bruk av følgende cloud/SDK shell kommando:\n",
        "\n",
        "```\n",
        "gsutil mb -b -c standard -l EUROPE-WEST3 gs://bucket-name\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYcemB6tdAG9"
      },
      "source": [
        "#### Laste opp filer til GCS bucket \n",
        "Laste opp lokale filer til GCS bucket ved bruk av cloud/SDK shell.\n",
        "\n",
        "```\n",
        "gsutil cp path/to/file.ndjson gs://bucket-name\n",
        "```\n",
        "\n",
        "Det er mulig å laste opp enkelte filter eller en liste med filter samtidig\n",
        "\n",
        "```\n",
        "gsutil cp path/to/*.ndjson gs://bucket-name\n",
        "```\n",
        "\n",
        "Til slutt verifiser at filene har blitt lastet opp, enten ved grafisk user interace (storage browser) eller gjennom cloud shell.\n",
        "\n",
        "```\n",
        "gsutil ls gs://bucket-name\n",
        "```\n",
        "\n",
        "En oversikt over gsutil kommandos finnes her: https://cloud.google.com/storage/docs/gsutil eller ved bruk av `gsutil help` i cloud shell terminal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSPrX2FQsyI2"
      },
      "source": [
        "#### Laste opp filer med klientbiblioteker.\n",
        "- Filer kan også lastes opp ved bruk av client bibliotheker. En oversikt over støttede programmeringsspråk finnes her: https://cloud.google.com/storage/docs/reference/libraries \n",
        "\n",
        "- I cellen under finnes det et minimalt eksempel med python client bibliothek.\n",
        "\n",
        "- For å kjøre python kode direkte fra denne notebook, må google cloud brukeren først autentiseres. \n",
        "\n",
        "- Vi bruker google.colab.auth python bibliothek til autentisering og google.cloud.storage til interaksjon med storage buckets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iE9_fDMveND"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "print('Authenticated') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0Vxzeugtgln"
      },
      "source": [
        "from google.cloud import storage\n",
        "from google.cloud import bigquery\n",
        "%load_ext google.colab.data_table\n",
        "\n",
        "proj = # TODO define workshop-project\n",
        "bucket =  # TODO define gs://bucket-name\n",
        "file_name = # TODO define lokal file name\n",
        "blob_name = # TODO define GCS blob name for the file\n",
        "client = storage.Client(project = proj) # initialize client and set the billing project. NB: vi trenger ikke å gjenta credentials her.\n",
        "bucket_object = client.get_bucket(bucket) # define the target bucket\n",
        "blob = bucket.blob(blob_name) # make a file blob\n",
        "blob.upload_from_filename(file_name) # upload the content from lokal file to GCS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXz3Q-1nx7oM"
      },
      "source": [
        "NB: Det finnes klientbiblioteker til de fleste GCP verktøy/tjenester. F.eks i python: https://cloud.google.com/python/docs/reference "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mmCYn_KpPVr"
      },
      "source": [
        "### 2. Importere data direkte fra GCS til BigQuery\n",
        "Etter dataene har blitt lastet opp til GCS skal de ofte importeres til BigQuery til videre analyse. BigQuery er et datavarehus som kan lagre store datasett hvor man kan bruke SQL spørringer for å hente ut data.\n",
        "\n",
        "BigQuery støtter en del formater direkte: \n",
        "- Avro\n",
        "- CSV\n",
        "- JSON\n",
        "- ORC\n",
        "- Parquet\n",
        "\n",
        "Dataene kan importeres fra GCS bøtter, lokale filer (max 10MB upload per fil), google drive, BigTable eller genereres on-the-fly (lage en tom tabell).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIfAvWpNwRyx"
      },
      "source": [
        "Her importerer vi dataene vi har lastet opp til GCS bøtte til BigQuery ved bruk av cloud shell kommando. BigQuery organiserer dataene i prosjekter, datasett og så tabeller. Før vi kan laste opp dataene i en tabell må vi derfor lage en ny datasett: (flytt denne til når vi skal kjøre dataflow jobben, siden det allerede finnes et raw dataset som er lagd med terraform?)\n",
        "\n",
        "```\n",
        "bq --location=EUROPE-WEST3 mk -d dataset_name\n",
        "```\n",
        "\n",
        "Når datasett har blitt opprettet kan vi importere dataene fra GCS inn til en BigQuery tabell under samme datasett.\n",
        "\n",
        "\n",
        "```\n",
        "bq --location=europe-west3 load --autodetect --source_format=NEWLINE_DELIMITED_JSON DATASET.TABLE_ID gs://path-to-file\n",
        "```\n",
        "\n",
        "***\n",
        "Do we want to load all trips into one table? Use e.g. file{1..3}.ndjson to upload more than one to its one file in one command\n",
        "***\n",
        "\n",
        "```\n",
        "bq --location=europe-west3 load --autodetect --source_format=NEWLINE_DELIMITED_JSON DATASET.TABLE_ID gs://path-to-data-folder/2021-04-14*.ndjson\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moEKqQ9_wR1X"
      },
      "source": [
        "#### Verifser import med en SQL spørring\n",
        "\n",
        "Før vi kan hente ut data fra BigQuery via SQL spørringer så må vi definere prosjekt, dataset og tabel IDene."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HWGHhyEDqfp"
      },
      "source": [
        "project_id = \"<prosjekt navn fra GCP>\"\n",
        "dataset_id = \"raw\"\n",
        "table_id = \"<table id>\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31xMoJcEEOne"
      },
      "source": [
        "project_id = \"saga-workshop-dtest-9hsr\"\n",
        "dataset_id = \"raw\"\n",
        "table_id = \"martin-test\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS3mZKqjfElZ"
      },
      "source": [
        "Her benytter vi oss av BigQuery \"inline magic\" kommandoer for å kjøre en SQL spørring direkte fra notebooken:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eap6zUDX8Fax"
      },
      "source": [
        "%%bigquery --project $project_id trip_df\n",
        "SELECT *\n",
        "FROM `<dataset_id>.<table_id>`"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqhAVecCfP2x"
      },
      "source": [
        "Her benytter vi oss av BigQuery klient biblioteket for å kjøre den samme SQL spørringen:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "527V0GV3OVo_"
      },
      "source": [
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "trip_df = client.query(\n",
        "    \"\"\"\n",
        "    SELECT *\n",
        "    FROM `%s.%s`\n",
        "    \"\"\" % (dataset_id, table_id)\n",
        ").to_dataframe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyahU54aNJyf"
      },
      "source": [
        "trip_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJruzauHEsJY"
      },
      "source": [
        "# Importe data med Dataflow inn i BigQuery\n",
        "\n",
        "TODO: Tekst her\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJFDux5pJs7P"
      },
      "source": [
        "!gcloud auth login"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyI73EO0RDFf"
      },
      "source": [
        "!gcloud projects list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUHxJlZwHT3P"
      },
      "source": [
        "!gcloud config set project saga-workshop-dtest-9hsr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umjXdOw7EQ6t"
      },
      "source": [
        "!gcloud auth application-default login"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPxEdqNz99v4"
      },
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install 'apache-beam[interactive]'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAwa8FHXRzWC"
      },
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install 'apache-beam[gcp]'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOhkIftTv8HY"
      },
      "source": [
        "!gcloud auth login"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zx7pxhYMQzot"
      },
      "source": [
        "!gcloud projects list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJhCZsYov8HZ"
      },
      "source": [
        "!gcloud config set project saga-workshop-dtest-9hsr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSSCJFRNv8HZ"
      },
      "source": [
        "!gcloud auth application-default login"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyHn7nYKvZh0"
      },
      "source": [
        "import csv\n",
        "\n",
        "# Test\n",
        "\n",
        "import apache_beam as beam\n",
        "import apache_beam.runners.interactive.interactive_beam as ib\n",
        "from apache_beam.io.gcp.gcsio import GcsIO\n",
        "import apache_beam.io.fileio\n",
        "from apache_beam.runners.interactive.interactive_runner import InteractiveRunner\n",
        "from apache_beam.options import pipeline_options\n",
        "from apache_beam.options.pipeline_options import GoogleCloudOptions\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OnBkheR91AG"
      },
      "source": [
        "options = pipeline_options.PipelineOptions()\n",
        "\n",
        "# Set the pipeline mode to stream the data from Pub/Sub.\n",
        "options.view_as(pipeline_options.StandardOptions).streaming = False\n",
        "options.view_as(GoogleCloudOptions).project = \"saga-workshop-dtest-9hsr\"\n",
        "\n",
        "p = beam.Pipeline(InteractiveRunner(), options=options)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjvInG2GAmbh"
      },
      "source": [
        "file_pattern = \"gs://saga-trafikkdata-prod-pz8l_timetrafikkdata-ingest/*\"\n",
        "\n",
        "file_lines = (p\n",
        "  | \"read\" >> beam.io.fileio.MatchFiles(file_pattern))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Lmtb144BIbV"
      },
      "source": [
        "ib.show(files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBmWABP4jSiS"
      },
      "source": [
        "foboie = (file_lines\n",
        "  | \"transform\" >> beam.Map(lambda line: csv.DictReader())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3yQixXVwR3q"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Cr1ugUh8_bs"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZL575pcI8_eW"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSXlYjwe8_g8"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWnVmhCcfbhU"
      },
      "source": [
        "output = \"Hello World\"\n",
        "!echo {output}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEL_z3zafbj7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMMPlWofvIkn"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_B82U21yxj5C"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ocw90BphDjth"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIm5GcbxDjwA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5FruVz5DjyQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rc5RdEYgDj0z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kt4byO94Dj3f"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyQOuGWMDj5_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXGcyQgrDj8l"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRuhwXeYDj_M"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lg_Mam6sDkBp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-3X5N5CDkES"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oM1W2_OTDkHK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPHZ-FacDkJr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrD5UMt-Xfhp"
      },
      "source": [
        "### Insert data with code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0DYAohwByWF"
      },
      "source": [
        "project_id = \"saga-workshop-dtest-9hsr\"\n",
        "dataset_id = \"raw\"\n",
        "table_id = \".\".join([project_id, dataset_id, \"martin_test_table\"])\n",
        "table_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3EWEm-Efbmm"
      },
      "source": [
        "from google.cloud import bigquery\n",
        "\n",
        "# Construct a BigQuery client object\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "client.create_table(table_id, exists_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUpIRlP5rayz"
      },
      "source": [
        "rows_to_insert = [\n",
        "    {u\"full_name\": u\"Phred Phlyntstone\", u\"age\": 32},\n",
        "    {u\"full_name\": u\"Wylma Phlyntstone\", u\"age\": 29},\n",
        "]\n",
        "\n",
        "errors = client.insert_rows_json(\n",
        "    table_id, rows_to_insert, row_ids=[None] * len(rows_to_insert)\n",
        ")  # Make an API request.\n",
        "if errors == []:\n",
        "    print(\"New rows have been added.\")\n",
        "else:\n",
        "    print(\"Encountered errors while inserting rows: {}\".format(errors))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7II1n7LyfbrY"
      },
      "source": [
        "from google.cloud import bigquery\n",
        "\n",
        "# Construct a BigQuery client object.\n",
        "client = bigquery.Client()\n",
        "\n",
        "# TODO(developer): Set table_id to the ID of the table to create.\n",
        "# table_id = \"your-project.your_dataset.your_table_name\"\n",
        "\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "    source_format=bigquery.SourceFormat.CSV, skip_leading_rows=1, autodetect=True,\n",
        ")\n",
        "\n",
        "with open(file_path, \"rb\") as source_file:\n",
        "    job = client.load_table_from_file(source_file, table_id, job_config=job_config)\n",
        "\n",
        "job.result()  # Waits for the job to complete.\n",
        "\n",
        "table = client.get_table(table_id)  # Make an API request.\n",
        "print(\n",
        "    \"Loaded {} rows and {} columns to {}\".format(\n",
        "        table.num_rows, len(table.schema), table_id\n",
        "    )\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwK1MS4CfbpD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}