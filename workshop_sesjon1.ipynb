{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "workshop_sesjon1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/svvsaga/datascience_workshop/blob/main/workshop_sesjon1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b68G62Ew84f"
      },
      "source": [
        "# Outline\n",
        "Her tar vi utgangspunkt i GPS-data fra Peder sin kjøretur fra Trondheim til Ørsta\n",
        "\n",
        "1. Laste opp fil til GCS via terminalen (Cloud Shell/gsutil)\n",
        "  - Nevne at det også kan gjøres via GUI og client libs\n",
        "  - Nevne rclone for større datamengder\n",
        "2. Importere data direkte fra GCS til BigQuery (Cloud Shell/bq)\n",
        "  - Nevne at det også kan gjøres via GUI og client libs\n",
        "3. Utføre en enkel BigQuery-spørring for å se at vi har fått inn data\n",
        "4. Legge til datamapping i en Dataflow-jobb, og importere og mappe data inn i BigQuery\n",
        "  - Kan bruke samme data som i pkt. 2, men at vi i DF-jobben forbedrer datamodellen, f.eks. ved å slå sammen to kolonner lat, long til en geography-kolonne\n",
        "  - Bør vise Dataflow-GUI mens jobben kjører\n",
        "5. Sjekke i BigQuery at dataene har komt inn på riktig format\n",
        "6. Vise at dei kan laste inn alle dataene frå GCS til BigQuery via GUI\n",
        "7. Seie litt om når Dataflow er riktig verktøy\n",
        "8. Meir? Skal vi joine med geolokasjon i denne bolken eller seinare?\n",
        "  - https://www.vegvesen.no/trafikkdata/api/?query=%7B%0A%20%20trafficRegistrationPoints%20%7B%0A%20%20%20%20id%0A%20%20%20%20name%0A%20%20%20%20location%20%7B%0A%20%20%20%20%20%20coordinates%20%7B%0A%20%20%20%20%20%20%20%20latLon%20%7B%0A%20%20%20%20%20%20%20%20%20%20lat%0A%20%20%20%20%20%20%20%20%20%20lon%0A%20%20%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20%7D%0A%20%20%20%20%7D%0A%20%20%7D%0A%7D%0A\n",
        "\n",
        "## Andre forslag:\n",
        "- Bruke python libs + dataframes + pandas til noe (sesjon 1 eller 2?)\n",
        "- Vurdere å vise Cloud Function som henter data og dytter det inn i Dataflow eller BigQuery?\n",
        "- Andre verktøy: AirByte, FME og DBT\n",
        "\n",
        "## Nytt forslag:\n",
        "- Ta utgangspunkt i trafikkdata (saga-trafikkdata-prod-pz8l_timetrafikkdata-ingest)\n",
        "- Gjere cirka det samme som før i steg 1, 2, 3\n",
        "- Då ser vi at dataene for det første er veldig nøsta og at vi ikkje veit \"ingest time\" som er ein del av filnamna.\n",
        "- Steg 4 (dataflow) vil då kunne gi verdi pga:\n",
        "  1. Kan ha med ingesttime inn i BigQuery\n",
        "  2. Kan hente ut berre totalt trafikkvolum, sidan vi ikkje bryr oss om resten"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xS9TeayuGQ-"
      },
      "source": [
        "TODO: Forklare at vi skal autentisere oss\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZxx7H7Iu8DX"
      },
      "source": [
        "### 1. Laste opp fil til GCS \n",
        "Det finnes flere måter å laster data inn til google cloud storage med ulike nivåer av kode-bruk:\n",
        "- gjennom google cloud platform grafisk user interface (nettleser)\n",
        "- gjennom google cloud shell (nettleser)\n",
        "- gjennom lokal installasjon av google cloud SDK\n",
        "- ved bruk av client blibliotheker (f.eks. python client)\n",
        "\n",
        "Her vil vi introdusere muligheter ved bruk av cloud shell og lokal installasjon av cloud SDK. \n",
        "\n",
        "Kommandoene er i utgangspunkt de samme, men oppsett er litt forskjellig: \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J31GmKgfY6Jt"
      },
      "source": [
        "#### Oppsett Cloud shell\n",
        "- Logg deg inn på GCP UI gjennom nettleseren din: https://console.cloud.google.com/ \n",
        "- Verifiser at prosjektet til workshop et valgt som arbeidsprosjekt\n",
        "- Aktiver cloud shell (\"Activate cloud shell\" knapp i øverste høyre hjørnet i UI), vent til cloud shell er provisjonert\n",
        "\n",
        "\n",
        "#### Oppsett Google cloud SDK\n",
        "- Last ned og installer Google cloud SDK for ditt system (i forkant av workshoppen): https://cloud.google.com/sdk/docs/install \n",
        "- initialiser cloud shell (verifiser at prosjektet er satt til ditt arbeids/workshop prosjekt)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UQ9Sp0zIHRP"
      },
      "source": [
        "#### Last ned eksempelfiler fra GCS (gjennom UI)\n",
        "- Last ned 2 filer fra\n",
        " `gs://saga-trafikkdata-prod-pz8l_timetrafikkdata-ingest`. Dette er en bøtte som finnes i prosjektet `saga-trafikkdata-prod-pz8l` og som dere har fått rettigheter til å laste ned filer fra.\n",
        "- Last ned de to filene:\n",
        " - `2021-03-23T09:31:31.945Z_2210582261344715.ndjson`\n",
        " - `2021-03-23T09:31:42.657Z_2210582990408255.ndjson`\n",
        "- Dette kan oppnås ved å bruke følgende cloud/SDK shell kommando:\n",
        "\n",
        "```\n",
        "gsutil cp gs://bucket-name path/to/file.csv\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJQgYDZPdAET"
      },
      "source": [
        "#### Lag en GCS bøtte\n",
        "Lag en GCS bøtte der filene skal lastest opp. \n",
        "Dette kan gjøres enten gjennom grafisk user interface eller gjennom cloud shell/SDK\n",
        "\n",
        "- I grafisk user interface naviger til Cloud Storage -> Browser og trykk på \"Create bucket\" \n",
        "- Velg et (globalt unik!) navn til bucket og konfigurer lagringsopsjoner\n",
        "  - standard storage\n",
        "  - single-region, europe-west3\n",
        "  - uniform access\n",
        "\n",
        "Det samme kan oppnås ved bruk av følgende cloud/SDK shell kommando:\n",
        "\n",
        "```\n",
        "gsutil mb -b -c standard -l EUROPE-WEST3 gs://bucket-name\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYcemB6tdAG9"
      },
      "source": [
        "#### Laste opp filer til GCS bucket \n",
        "Laste opp lokale filer til GCS bucket ved bruk av cloud/SDK shell.\n",
        "\n",
        "```\n",
        "gsutil cp path/to/file.ndjson gs://bucket-name\n",
        "```\n",
        "\n",
        "Det er mulig å laste opp enkelte filter eller en liste med filter samtidig\n",
        "\n",
        "```\n",
        "gsutil cp path/to/*.ndjson gs://bucket-name\n",
        "```\n",
        "\n",
        "Til slutt verifiser at filene har blitt lastet opp, enten ved grafisk user interace (storage browser) eller gjennom cloud shell.\n",
        "\n",
        "```\n",
        "gsutil ls gs://bucket-name\n",
        "```\n",
        "\n",
        "En oversikt over gsutil kommandos finnes her: https://cloud.google.com/storage/docs/gsutil eller ved bruk av `gsutil help` i cloud shell terminal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mmCYn_KpPVr"
      },
      "source": [
        "### 2. Importere data direkte fra GCS til BigQuery\n",
        "Etter dataene har blitt lastet opp til GCS skal de ofte importeres til BigQuery til videre analyse. BigQuery er et datavarehus som kan lagre store datasett hvor man kan bruke SQL spørringer for å hente ut data.\n",
        "\n",
        "BigQuery støtter en del formater direkte: \n",
        "- Avro\n",
        "- CSV\n",
        "- JSON\n",
        "- ORC\n",
        "- Parquet\n",
        "\n",
        "Dataene kan importeres fra GCS bøtter, lokale filer (max 10MB upload per fil), google drive, BigTable eller genereres on-the-fly (lage en tom tabell).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIfAvWpNwRyx"
      },
      "source": [
        "Her vil vi importere dataene vi har lastet opp til en GCS bøtte til BigQuery ved bruk av en cloud shell kommando. BigQuery organiserer dataene i prosjekter, datasett og så tabeller. Før vi kan laste opp dataene i en tabell må vi derfor lage en ny datasett. \n",
        "\n",
        "Alle BigQuery relaterte kommandoene i cloud shell har følgende syntax:\n",
        "\n",
        "```\n",
        "bq [--global_flags] <command> [--command_flags] [args]\n",
        "```\n",
        "\n",
        "\n",
        "Vi begynner med å lage et datasett i samme lokasjon (single-region, europe-west3) som google cloud storage bøtte:\n",
        "\n",
        "\n",
        "```\n",
        "bq --location=EUROPE-WEST3 mk -d dataset_name\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "Når datasett har blitt opprettet kan vi importere dataene fra GCS inn til en BigQuery tabell: `dataset_name.table_name`\n",
        "\n",
        "\n",
        "```\n",
        "bq --location=europe-west3 load --autodetect --source_format=NEWLINE_DELIMITED_JSON <DATASET>.<TABLE_ID> gs://<bucket_name>/2021-03-23T09:31:31.945Z_2210582261344715.ndjson\n",
        "```\n",
        "\n",
        "Det er også mulig å laste opp alle filer samtidig ved bruk av `*`:\n",
        "\n",
        "```\n",
        "bq --location=europe-west3 load --autodetect --source_format=NEWLINE_DELIMITED_JSON DATASET.TABLE_ID gs://<bucket_name>/*.ndjson\n",
        "```\n",
        "\n",
        "`--autodetect` er en flag som la BigQuery velge selv hvilken data type de ulike felter sannsynligvis har. Dette gjør ofte en rimelig bra jobb i første omgang og er veldig nyttig når man laster inn filer i JSON format som kan ha en kompleks, nestet struktur.\n",
        "\n",
        "Alternativt er det mulig å angi en custom schema etter filnavn. Her kan det brukes en JSON-fil som definerer schema, eller en komma-separarert string av format `FELT_NAVN:DATA_TYPE,...`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moEKqQ9_wR1X"
      },
      "source": [
        "#### Verifser import med en SQL spørring\n",
        "\n",
        "Før vi kan hente ut data fra BigQuery via SQL spørringer så må vi definere prosjekt, dataset og tabel IDene."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HWGHhyEDqfp"
      },
      "source": [
        "project_id = \"<prosjekt navn fra GCP>\"\n",
        "dataset_id = \"workshop\"\n",
        "table_id = \"<table id>\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLTxWvvXlyLR"
      },
      "source": [
        "Vi benytte oss av BigQuery \"inline magic\" kommandoer for å kjøre en SQL spørring direkte fra notebooken. Det krever autentisering mot GCP først:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlGvAH4Tl87r"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "print('Authenticated') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS3mZKqjfElZ"
      },
      "source": [
        "Her benytter vi oss av BigQuery \"inline magic\" kommandoer for å kjøre en SQL spørring direkte fra notebooken. Uheldigvis støtter ikke denne metoden å bytte ut \"dataset_id\" og \"table_id\" automatisk, så disse må du lime inn selv."
        "Inline magic kommandoer kjøres ved bruk av `%%` øverst i notebook cellen. I tillegg angir vi et BigQuery prosjekt og en lokal python variable dataene skal lagres in."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eap6zUDX8Fax"
      },
      "source": [
        "%%bigquery --project $project_id trip_df\n",
        "SELECT *\n",
        "FROM `<dataset_id>.<table_id>`"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqhAVecCfP2x"
      },
      "source": [
        "Her gjør vi det samme som over, men nå benytter vi oss av BigQuery klientbiblioteket for å kjøre den samme SQL spørringen:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "527V0GV3OVo_"
      },
      "source": [
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "trip_df = client.query(\n",
        "    \"\"\"\n",
        "    SELECT *\n",
        "    FROM `%s.%s`\n",
        "    \"\"\" % (dataset_id, table_id)\n",
        ").to_dataframe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyahU54aNJyf"
      },
      "source": [
        "trip_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJruzauHEsJY"
      },
      "source": [
        "# Importere data med Dataflow inn i BigQuery\n",
        "\n",
        "TODO: Tekst her\n",
        "\n",
        "Først må vi installere Apache Beam i notebooken. Husk å trykke \"Restart runtime\" etter å ha installert python-modulene. Dette er nødvendig for at notebooken skal kunne bruke nyinstallerte moduler."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPxEdqNz99v4"
      },
      "source": [
        "!pip install 'apache-beam[interactive]' # For running beam interactively\n",
        "!pip install 'apache-beam[gcp]'         # For running beam on GCP Dataflow\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6oJvw2nk1lR"
      },
      "source": [
        "# TODO: Gjere dette i starten\n",
        "%env PROJECT_ID=saga-workshop-dtest-9hsr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0FOfn4riQHa"
      },
      "source": [
        "Vi må også installere Apache Beam-pakker tilpasset bruk med GCP. Igjen, husk å trykke \"Restart runtime\" etterpå."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJFDux5pJs7P"
      },
      "source": [
        "# Ta vekk\n",
        "!gcloud auth login"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zx7pxhYMQzot"
      },
      "source": [
        "# Ta vekk\n",
        "!gcloud projects list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUHxJlZwHT3P"
      },
      "source": [
        "!gcloud config set project $PROJECT_ID"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umjXdOw7EQ6t"
      },
      "source": [
        "!gcloud auth application-default login"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyHn7nYKvZh0"
      },
      "source": [
        "import csv\n",
        "import os\n",
        "\n",
        "# Test\n",
        "\n",
        "import apache_beam as beam\n",
        "import apache_beam.runners.interactive.interactive_beam as ib\n",
        "import apache_beam.io.fileio\n",
        "from apache_beam.runners.interactive.interactive_runner import InteractiveRunner\n",
        "from apache_beam.options import pipeline_options\n",
        "from apache_beam.options.pipeline_options import GoogleCloudOptions\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OnBkheR91AG"
      },
      "source": [
        "options = pipeline_options.PipelineOptions()\n",
        "\n",
        "project_id = os.environ['PROJECT_ID']\n",
        "\n",
        "# Set the pipeline mode to stream the data from Pub/Sub.\n",
        "options.view_as(pipeline_options.StandardOptions).streaming = False\n",
        "options.view_as(GoogleCloudOptions).project = project_id\n",
        "\n",
        "p = beam.Pipeline(InteractiveRunner(), options=options)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5yiF2ZLF0ZT"
      },
      "source": [
        "Dere skal nå få lov å sette sammen noen enkle Beam-pipelines selv. Under har vi inkludert en liste med 8 transformasjoner som utgjør deres \"byggesett\". Dere må selv velge ut de nødvendige transformasjonene basert på hva oppgaven spør etter.\n",
        "\n",
        "- Finne filer som matcher et gitt mønster: `beam.io.fileio.MatchFiles(pattern))`\n",
        "- Filtrere vekk elementer basert på en betingelse: `beam.Filter(condition_check_function)`\n",
        "- Hente ut N tilfeldige elementer fra en større samling (PCollection): `beam.combiners.Sample.FixedSizeGlobally(N)`\n",
        "- Gjøre om filstier til faktisk lesbare filer: `beam.io.fileio.ReadMatches()`\n",
        "- Utføre en egendefinert transformasjon på hvert element. Passer best når output av transformasjonen er et enkelt element: `beam.Map(transform_function)`\n",
        "- Utføre en egendefinert transformasjon på hvert element, hvor transformasjonen skal resultere i en liste med output-elementer. Denne vil da slå sammen alle output-elementer fra hver transformasjon i en felles liste: `beam.FlatMap(transform_function)`\n",
        "\n",
        "En fullstendig liste med innebygde transformasjoner kan sees her: https://beam.apache.org/documentation/transforms/python/overview/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjvInG2GAmbh"
      },
      "source": [
        "file_pattern = \"gs://saga-trafikkdata-prod-pz8l_timetrafikkdata-ingest/2021-04-10*\" # April 10th, in the first 10 minutes of the 10th hour\n",
        "\n",
        "file_paths = (p\n",
        "  | \"find files\" >> beam.io.fileio.MatchFiles(file_pattern))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8F-qttfF6cAd"
      },
      "source": [
        "Vi har nå definert en minimal pipeline som finner filstier på alle filer i GCS som matcher mønsteret i `file_pattern`. For å sjekke at dette fungerer som det skal, kan vi bruke `ib.show`, som vi får fra den interaktive modulen til Apache Beam.\n",
        "\n",
        "Men vi har ikke lyst å liste ut absolutt alle filene som matcher mønsteret. La oss heller plukke ut 10 tilfeldige filstier. Dette gjør vi med den innebygde \"sample\"-transformasjonen `beam.combiners.Sample.FixedSizeGlobally(n)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Lmtb144BIbV"
      },
      "source": [
        "file_samples = file_paths | \"sample 10 files\" >> beam.combiners.Sample.FixedSizeGlobally(10)\n",
        "ib.show(file_samples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyWDDrIg7V-O"
      },
      "source": [
        "Når vi har bekreftet at vi ser filstiene kan vi fortsette med pipelinen vår. I koden under har vi lagt til tre ekstra steg:\n",
        "\n",
        "1. Et steg som klargjør filene til lesing\n",
        "2. Et steg som transformerer innholdet i filene til en liste av python dictionaries\n",
        "3. Et steg som skriver python dictionaries til BigQuery\n",
        "\n",
        "Funksjonen `transform_file_to_objects` inneholder innmaten i steg 2. Slik den er nå vil den bare parse hver rad i filene til en python dictionary som gjenspeiler json-strukturen. Dette betyr at foreløpig vil pipelinen resultere i den samme datamodellen/skjema i BigQuery som vi fikk når vi lastet opp json-filene direkte.\n",
        "\n",
        "Vi ønsker som sagt å også få med \"ingest time\", altså tidspunktet for vi mottok filene. Dette tidspunktet er en del av filnavnet. Dette ser vi dersom vi inspiserer filstiene vi fikk i steget over.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qFi7AYei5QP"
      },
      "source": [
        "import json\n",
        "\n",
        "# This function transforms each line of an .ndjson file into a python dictionary.\n",
        "# A python dictionary is a record type with keys and values, and is therefore a good fit for json data\n",
        "# Later, we will send our dictionaries to Apache Beam's BigQuery library\n",
        "def transform_file_to_objects(file_path, file_contents):\n",
        "  json_objects = []\n",
        "  lines = file_contents.split('\\n')\n",
        "  for line in lines:\n",
        "    if line and line.startswith(\"{\"):\n",
        "        # We use json.loads to parse each line (which contains json) into a python dictionary\n",
        "        timestrafikkdata_row = json.loads(line)\n",
        "        # timestamp_string = regex(...)\n",
        "        #\n",
        "        # TODO: Extract ingest time from the file_path parameter available in this function, and add it to timestrafikkdata_row\n",
        "        # \"Transform timestamp to date type and add it to timestrafikkdata_row\"\n",
        "        #\n",
        "        # timestamp = Date(timestamp_string)\n",
        "        # timestrafikkdata_row[\"ingest_time\"] = timestamp\n",
        "        json_objects.append(timestrafikkdata_row)\n",
        "  return json_objects\n",
        "\n",
        "# file.read_utf8()\n",
        "\n",
        "write_results = (file_paths\n",
        "  | \"prepare files for reading\" >> beam.io.fileio.ReadMatches()\n",
        "  | \"transform to objects\" >> beam.FlatMap(lambda file: transform_file_to_objects(file.metadata.path, file.read_utf8()))\n",
        "  | \"write to BigQuery\" >> beam.io.WriteToBigQuery(\"saga-workshop-dtest-9hsr:workshop.df4\",\n",
        "                                                   schema=beam.io.gcp.bigquery.SCHEMA_AUTODETECT,\n",
        "                                                   write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,\n",
        "                                                   create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
        "                                                   custom_gcs_temp_location=\"gs://saga-workshop-dtest-9hsr_dataflow_files/temp\",\n",
        "                                                   method=beam.io.gcp.bigquery.WriteToBigQuery.Method.FILE_LOADS\n",
        "                                                   ))\n",
        "\n",
        "# p.run().wait_until_finish()\n",
        "\n",
        "#  | \"extract contents\" >> beam.Map(lambda file: file.read_utf8()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-TDKGtlpRCv"
      },
      "source": [
        "from apache_beam.runners import DataflowRunner\n",
        "from apache_beam.options.pipeline_options import WorkerOptions\n",
        "\n",
        "df_options = pipeline_options.PipelineOptions()\n",
        "\n",
        "# Set the project to the default project in your current Google Cloud\n",
        "# environment.\n",
        "df_options.view_as(GoogleCloudOptions).project = project_id\n",
        "df_options.view_as(GoogleCloudOptions).region = 'europe-west4'\n",
        "df_options.view_as(GoogleCloudOptions).service_account_email = 'dataflow@{}.iam.gserviceaccount.com'.format(project_id)\n",
        "df_options.view_as(WorkerOptions).network = 'vpc-network'\n",
        "#df_options.view_as(GoogleCloudOptions).subnetwork = 'regions/europe-west4/subnetworks/vpc-network'\n",
        "\n",
        "# Storage for uploading Dataflow code and temporary files\n",
        "dataflow_gcs_location = 'gs://saga-workshop-dtest-9hsr_dataflow_files'\n",
        "df_options.view_as(GoogleCloudOptions).staging_location = '%s/staging' % dataflow_gcs_location\n",
        "df_options.view_as(GoogleCloudOptions).temp_location = '%s/temp' % dataflow_gcs_location\n",
        "\n",
        "runner = DataflowRunner()\n",
        "runner.run_pipeline(p, options=df_options)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AWD55diV55k"
      },
      "source": [
        "## Ekstramateriale"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSPrX2FQsyI2"
      },
      "source": [
        "#### Laste opp filer med klientbiblioteker.\n",
        "- Filer kan også lastes opp ved bruk av client bibliotheker. En oversikt over støttede programmeringsspråk finnes her: https://cloud.google.com/storage/docs/reference/libraries \n",
        "\n",
        "- I cellen under finnes det et minimalt eksempel med python client bibliothek.\n",
        "\n",
        "- For å kjøre python kode direkte fra denne notebook, må google cloud brukeren først autentiseres. \n",
        "\n",
        "- Vi bruker google.colab.auth python bibliothek til autentisering og google.cloud.storage til interaksjon med storage buckets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iE9_fDMveND"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "print('Authenticated') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0Vxzeugtgln"
      },
      "source": [
        "from google.cloud import storage\n",
        "from google.cloud import bigquery\n",
        "%load_ext google.colab.data_table\n",
        "\n",
        "proj = # TODO define workshop-project\n",
        "bucket =  # TODO define gs://bucket-name\n",
        "file_name = # TODO define lokal file name\n",
        "blob_name = # TODO define GCS blob name for the file\n",
        "client = storage.Client(project = proj) # initialize client and set the billing project. NB: vi trenger ikke å gjenta credentials her.\n",
        "bucket_object = client.get_bucket(bucket) # define the target bucket\n",
        "blob = bucket.blob(blob_name) # make a file blob\n",
        "blob.upload_from_filename(file_name) # upload the content from lokal file to GCS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXz3Q-1nx7oM"
      },
      "source": [
        "NB: Det finnes klientbiblioteker til de fleste GCP verktøy/tjenester. F.eks i python: https://cloud.google.com/python/docs/reference "
      ]
    }
  ]
}