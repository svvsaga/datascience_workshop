{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "workshop_sesjon1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/svvsaga/datascience_workshop/blob/main/workshop_sesjon1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xS9TeayuGQ-"
      },
      "source": [
        "# Sesjon 1: \"Extract and Load\" med GCS og BigQuery\n",
        "\n",
        "I denne sesjonen skal vi lære grunnleggende operasjoner for å få data inn i GCS og BigQuery. Vi ser også på hvordan vi kan hente data fra et API, og dytte disse inn i BigQuery."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZxx7H7Iu8DX"
      },
      "source": [
        "# 1) Laste opp fil til GCS\n",
        "\n",
        "For både GCS og BigQuery finnes det mange måter å bruke disse på. De typiske alternativene er:\n",
        "- gjennom GCP cloud console (grafisk grensesnitt i nettleser)\n",
        "- gjennom Google Cloud Shell (terminal i nettleser)\n",
        "- gjennom lokal installasjon av terminalverktøyet Google Cloud SDK\n",
        "- ved bruk av klientbliblioteker (f.eks. python-klient).\n",
        "\n",
        "I stegene under har vi lagt opp til at du kan bruke enten grafisk grensesnitt eller kommandolinjeverktøy. Obs: Du skal **ikke** utføre oppgavene på begge måter - det får du ikke tid til. Vi anbefaler at du velger den arbeidsmåten som du er mest komfortabel med. **Dersom du er usikker, velg det grafiske grensesnittet.**\n",
        "\n",
        "Dersom du ønsker å bruke kommandolinja, har vi under skrevet opp stegene for å klargjøre kommandolinja for bruk med GCP.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-AVdy_mOYC0"
      },
      "source": [
        "## Autentisering for bruk i python-kode\n",
        "\n",
        "Vi skal senere kjøre python-kode som krever at du er autentisert mot GCP. Kjør kode-cellen under for å bli autentisert (trykk på play-knappen)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fn0pLhpOdxJ"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "print('Authenticated') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_0xAVCLRcZf"
      },
      "source": [
        "Mens vi er i gang med oppsett kan vi like gjerne også definere prosjekt-IDen din i koden med en gang. Finn fram det fulle prosjektnavnet på prosjektet du har fått utdelt, og lim det inn under."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cokpZ11Rn4Q"
      },
      "source": [
        "project_id = \"<prosjekt-navn i GCP>\" # Endre denne\n",
        "dataset_id = \"workshop\"              # Ikke endre denne"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J31GmKgfY6Jt"
      },
      "source": [
        "## Oppsett av kommandolinja (valgfri)\n",
        "\n",
        "Dersom du ønsker å utføre stegene i denne notebooken via kommandolinja, utfør en av oppsettene under. Du trenger ikke å utføre begge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7lhUGJTtt8M"
      },
      "source": [
        "### Oppsett Cloud shell\n",
        "- Logg deg inn på GCP UI gjennom nettleseren din: https://console.cloud.google.com/ \n",
        "- Verifiser at prosjektet til workshop et valgt som arbeidsprosjekt\n",
        "- Aktiver cloud shell (\"Activate cloud shell\" knapp i øverste høyre hjørnet i UI), vent til cloud shell er provisjonert"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vL-wiUYpQwvd"
      },
      "source": [
        "### Oppsett Google cloud SDK\n",
        "- Last ned og installer Google cloud SDK for ditt system (i forkant av workshoppen): https://cloud.google.com/sdk/docs/install \n",
        "- initialiser cloud shell (verifiser at prosjektet er satt til ditt arbeids/workshop prosjekt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UQ9Sp0zIHRP"
      },
      "source": [
        "## 1.1) Last ned eksempelfiler fra GCS\n",
        "\n",
        "### Oppgaven\n",
        "\n",
        "Last ned 2 filer fra\n",
        " `gs://saga-workshop-eksempelfiler`. Dette er en bøtte som finnes i prosjektet `saga-workshop-data-vu8x` og som dere har fått rettigheter til å laste ned filer fra.\n",
        "\n",
        "### Med web-grensesnittet\n",
        "\n",
        "- Velg prosjektet som dataene ligger i (`saga-workshop-data-vu8x`). Naviger deg så til \"Storage\", f.eks. ved å bruke søkefeltet på toppen. Finn så den riktige bøtta (`gs://saga-workshop-eksempelfiler`).\n",
        "- Last ned de to timestrafikkdata-filene som ligger der:\n",
        " - `2021-03-23T09:31:31.945Z_2210582261344715.ndjson`\n",
        " - `2021-03-23T09:31:42.657Z_2210582990408255.ndjson`\n",
        "\n",
        "### Med kommandolinja\n",
        "\n",
        "Du kan bruke følgende kommando i kommandolinja. Husk å bytte ut `bucket-name` med bøtta du vil lese fra.\n",
        "\n",
        "```bash\n",
        "# For å liste ut filene\n",
        "gsutil ls gs://bucket-name\n",
        "\n",
        "# For å laste ned filene til inneværende mappe\n",
        "gsutil cp gs://bucket-name/*.ndjson .\n",
        "```\n",
        "\n",
        "**Tips:** Ikke bruk `gsutil ls` til å liste ut alle filene i bøtter som har over 100 filer. Det vil ta laaang tid!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJQgYDZPdAET"
      },
      "source": [
        "## 1.2) Lag en GCS bøtte\n",
        "\n",
        "### Oppgaven\n",
        "\n",
        "Lag en GCS bøtte der filene skal lastes opp. \n",
        "\n",
        "### Med web-grensesnittet\n",
        "\n",
        "- I web-grensesnittet, naviger til Cloud Storage og trykk på \"Create bucket\" \n",
        "- Velg et (globalt unik!) navn til bucket og konfigurer lagringsopsjoner\n",
        "  - standard storage\n",
        "  - multi-region, eu\n",
        "  - uniform access\n",
        "\n",
        "### Med kommandolinja\n",
        "Du kan bruke følgende kommando:\n",
        "\n",
        "```\n",
        "gsutil mb -b on -c standard -l EU gs://bucket-name\n",
        "\n",
        "# \"-b on\" turns on uniform bucket access\n",
        "# \"-c standard\" sets the standard storage class\n",
        "# \"-l EU\" sets storage location to europe (multi-region)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYcemB6tdAG9"
      },
      "source": [
        "## 1.3) Laste opp filer til GCS bucket\n",
        "\n",
        "### Oppgaven\n",
        "\n",
        "Vi skal nå laste opp filene som vi lastet ned tidligere. Etterpå må du huske å dobbeltsjekke at filene faktisk har dukket opp i bøtta.\n",
        "\n",
        "### Med web-grensesnittet\n",
        "\n",
        "Prøv deg fram. Grensesnittet er relativt intuitivt, så dette får du til!\n",
        "\n",
        "### Med kommandolinja\n",
        "\n",
        "```\n",
        "gsutil cp path/to/file.ndjson gs://bucket-name\n",
        "```\n",
        "\n",
        "Det er mulig å laste opp enkelte filter eller en liste med filter samtidig\n",
        "\n",
        "```\n",
        "gsutil cp path/to/*.ndjson gs://bucket-name\n",
        "```\n",
        "\n",
        "For å verifisere at filene har dukket opp:\n",
        "\n",
        "```\n",
        "gsutil ls gs://bucket-name\n",
        "```\n",
        "\n",
        "En oversikt over gsutil kommandos finnes her: https://cloud.google.com/storage/docs/gsutil eller ved bruk av `gsutil help` i cloud shell terminal.\n",
        "\n",
        "### Tips: Opplasting av store datamengder til GCS\n",
        "\n",
        "På Saga-prosjektet har vi opplevd at det å bruke gsutil fungerer greit til middels store opplastinger til GCS - opptil noen titals GB. For større opplastninger vil gsutil være treigt. Da bør du bruke [rclone, som håndterer slike opplastninger mye bedre.](https://rclone.org/googlecloudstorage/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mmCYn_KpPVr"
      },
      "source": [
        "# 2) Importere data direkte fra GCS til BigQuery\n",
        "\n",
        "Etter at dataene har blitt lastet opp til GCS skal de ofte importeres til BigQuery for videre analyse.\n",
        "\n",
        "BigQuery støtter en del formater direkte: \n",
        "- Avro\n",
        "- CSV\n",
        "- JSON\n",
        "- ORC\n",
        "- Parquet\n",
        "\n",
        "Dataene kan importeres fra GCS-bøtter, lokale filer (max 10MB upload per fil), Google Drive, BigTable eller genereres on-the-fly (lage en tom tabell).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIfAvWpNwRyx"
      },
      "source": [
        "## 2.1) Opprette et BigQuery-datasett\n",
        "\n",
        "### Oppgaven\n",
        "\n",
        "BigQuery organiserer dataene i prosjekter, datasett og så tabeller. Før vi kan laste opp dataene i en tabell må vi derfor lage et nytt datasett.\n",
        "\n",
        "**Det nye datasettet skal ha navnet workshop.**\n",
        "\n",
        "### Med web-grensesnittet\n",
        "\n",
        "Via web-grensesnittet kan dette gjøres ved å først navigere deg inn i BigQuery, velge ditt prosjekt og så klikke på \"Create dataset\". Location skal være den samme som for bøtta vi lagde, altså multi-region, EU.\n",
        "\n",
        "### Med kommandolinja\n",
        "Dersom du vil bruke kommandolinja, bruk følgende kommando.\n",
        "\n",
        "```\n",
        "bq --location=EU mk -d dataset_name\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CqV799GwP7e"
      },
      "source": [
        "## 2.2) Importere data inn i ny tabell\n",
        "\n",
        "### Oppgaven\n",
        "\n",
        "Vi kan nå importere dataene fra GCS til en BigQuery-tabell: `dataset_name.table_name`, hvor \"dataset_name\" altså skal være workshop. Et forslag til tabellnavn er \"timestrafikkdata\".\n",
        "\n",
        "### Med web-grensesnittet\n",
        "\n",
        "I web-grensesnittet, velg datasettet \"workshop\" og trykk på \"Create table\". For å velge datasettet kan det være nødvendig å trykke på de tre prikkene til høyre for datasett-navnet, og velge \"open\".\n",
        "\n",
        "Velg at filene skal lastes opp fra GCS. Gå via \"Browse\"-knappen for å finne bøtta og filene du vil laste opp. \"Browse\"-dialogen får det til å virke som at man må velge en fil, men det er i stedet mulig å skrive inn `*` i \"name\"-feltet for å matche alle filer, eller `*.ndjson` for å matche alle filer som slutter på `.ndjson`. Gjør dette slik at du får lastet opp begge filene.\n",
        "\n",
        "Deretter kan du velge et tabellnavn (f.eks. \"timestrafikkdata\"). I tillegg må du huke av for \"auto detect\" for skjemaet. Deretter kan du velge \"Create table\".\n",
        "\n",
        "### Med kommandolinja\n",
        "\n",
        "Dersom du heller vil bruke kommandolinja, bruk følgende kommando for å laste opp alle ndjson-filer fra kildebøtta:\n",
        "\n",
        "```\n",
        "bq --location=eu load --autodetect --source_format=NEWLINE_DELIMITED_JSON <DATASET>.<TABLE_ID> gs://<bucket_name>/*.ndjson\n",
        "```\n",
        "\n",
        "`--autodetect` er en flag som la BigQuery velge selv hvilken data type de ulike felter sannsynligvis har. Dette gjør ofte en rimelig bra jobb i første omgang og er veldig nyttig når man laster inn filer i JSON format som kan ha en kompleks, nestet struktur.\n",
        "\n",
        "Alternativt er det mulig å angi en custom schema etter filnavn. Her kan det brukes en JSON-fil som definerer schema, eller en komma-separarert string av format `FELT_NAVN:DATA_TYPE,...`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moEKqQ9_wR1X"
      },
      "source": [
        "## 2.3) Verifiser import med en SQL spørring\n",
        "\n",
        "### Oppgaven\n",
        "\n",
        "Nå må vi verifisere at importen ble vellykket.\n",
        "\n",
        "### Med web-grensesnittet\n",
        "\n",
        "Gjennom web-grensesnittet skal du nå kunne se en ny tabell. I alle fall dersom du oppdaterer siden. Verifiser at denne inneholder data ved å opprette og kjøre følgende SQL-spørring. Du må selv angi `<table_id>`.\n",
        "\n",
        "```\n",
        "SELECT *\n",
        "FROM `workshop.<table_id>`\n",
        "```\n",
        "\n",
        "### Med kommandolinja (egentlig via Python-kode)\n",
        "\n",
        "I denne seksjonen gjør vi en vri. Vi viser ikke hvordan du utfører en BigQuery-spørring via kommandolinja (men du kan gjøre det om du vil). I stedet viser vi hvordan du gjør det rett fra denne notebooken, både via å kjøre python-kode, og via ipython \"inline magic\".\n",
        "\n",
        "Før vi kan hente ut data fra BigQuery via SQL-spørringer så må vi definere korrekt tabell-ID (altså navnet på tabellen)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HWGHhyEDqfp"
      },
      "source": [
        "from google.cloud import bigquery\n",
        "\n",
        "table_id = \"<table id>\"              # Endre denne til navnet på tabellen din"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS3mZKqjfElZ"
      },
      "source": [
        "Her benytter vi oss av BigQuery \"inline magic\" kommandoer for å kjøre en SQL spørring direkte fra notebooken. Uheldigvis støtter ikke denne metoden å bytte ut \"dataset_id\" og \"table_id\" automatisk, så disse må du lime inn selv. Inline magic kommandoer kjøres ved bruk av `%%` øverst i notebook cellen. I tillegg angir vi et BigQuery prosjekt og en lokal python variable dataene skal lagres in. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eap6zUDX8Fax"
      },
      "source": [
        "%%bigquery --project $project_id\n",
        "SELECT *\n",
        "FROM `workshop.<table id>`"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqhAVecCfP2x"
      },
      "source": [
        "Her gjør vi det samme som over, men nå benytter vi oss av BigQuery klientbiblioteket for å kjøre den samme SQL spørringen:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "527V0GV3OVo_"
      },
      "source": [
        "client = bigquery.Client(project=project_id)\n",
        "client.query(\n",
        "    \"\"\"\n",
        "    SELECT *\n",
        "    FROM `%s.%s`\n",
        "    \"\"\" % (dataset_id, table_id)\n",
        ").to_dataframe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYbnJdG_p8gz"
      },
      "source": [
        "## 2.4) Automatisk import fra GCS til BigQuery er ganske rå!\n",
        "\n",
        "I skrivende stund har vi 370 GB++ med timesaggregert trafikkdata i bøtta `gs://saga-workshop-timestrafikkdata-v0` som vi så på tidligere. I denne oppgaven skal vi vise at det er null problem å laste opp hele dette datasettet til BigQuery, og det tar ikke engang særlig lang tid.\n",
        "\n",
        "I web-grensesnittet, naviger deg tilbake til workshop-datasettet, og velg \"Create table\". Igjen skal datakilden være GCS. \"Browse\"-knappen lar deg ikke velge bøtter som ligger i andre prosjekter enn det du jobber i nå. Derfor må du denne gangen selv fylle inn `gs://saga-workshop-timestrafikkdata-v0/*.ndjson`. Ellers skal vi gjøre det samme som sist gang. Velg deg et tabellnavn og huk av for \"auto detect\" på skjema. Trykk så \"Create table\". Dette vil antakelig ta rundt 3-4 minutter. Du kan følge med på importjobbens status ved å navigere deg inn på \"job history\".\n",
        "\n",
        "For å gjøre dette med kommandolinja kan vi gjøre akkurat det samme som sist gang, foruten at kildebøtta er ulik. Kommandoen blir altså noe slikt som:\n",
        "\n",
        "```\n",
        "bq --location=eu load --autodetect --source_format=NEWLINE_DELIMITED_JSON <DATASET>.<TABLE_ID> gs://<bucket_name>/*.ndjson\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tG8SpmlwlHDb"
      },
      "source": [
        "# 3) Importer data fra API\n",
        "\n",
        "Av og til er det data fra et API man ønsker å utforske i BigQuery. Her viser vi hvordan man kan gjøre dette. Denne gangen skal vi ta en titt på [data fra trafikkdata-APIet](https://www.vegvesen.no/trafikkdata/api/?query=%7B%0A%20%20trafficRegistrationPoints%20%7B%0A%20%20%20%20id%0A%20%20%20%20name%0A%20%20%20%20location%20%7B%0A%20%20%20%20%20%20coordinates%20%7B%0A%20%20%20%20%20%20%20%20latLon%20%7B%0A%20%20%20%20%20%20%20%20%20%20lat%0A%20%20%20%20%20%20%20%20%20%20lon%0A%20%20%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20%7D%0A%20%20%20%20%7D%0A%20%20%7D%0A%7D%0A). Trykk på \"play\"-knappen for å hente dataene i nettleseren. Her ser vi altså at dataene består av id, navn og posisjon for landets trafikkregistreringspunkter. Disse dataene skal vi bruke senere for å berike timestrafikkdataene med posisjon.\n",
        "\n",
        "I denne seksjonen bruker vi python som kan kjøres rett i notebooken.\n",
        "\n",
        "Først skal vi installere en \"pre-release\"-versjon av pakken gql, som lar oss gjøre spørringer mot API-et via GraphQL."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzhqG5WblHQZ"
      },
      "source": [
        "!pip install --pre gql[all]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHq6ytoRPnvF"
      },
      "source": [
        "Så utfører vi det faktiske API-kallet, og printer det første trafikkregistreringspunktet i responsen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWHuAcG9nFEv"
      },
      "source": [
        "import json\n",
        "from gql import gql, Client\n",
        "from gql.transport.aiohttp import AIOHTTPTransport\n",
        "from gql.transport.requests import RequestsHTTPTransport\n",
        "\n",
        "# Setup connection\n",
        "transport = RequestsHTTPTransport(url=\"https://www.vegvesen.no/trafikkdata/api/\", verify=True, retries=3)\n",
        "\n",
        "# Create a GraphQL client using the defined connection\n",
        "gql_client = Client(transport=transport, fetch_schema_from_transport=True)\n",
        "\n",
        "# Provide a GraphQL query\n",
        "query = gql(\n",
        "    \"\"\"\n",
        "    {\n",
        "      trafficRegistrationPoints {\n",
        "        id\n",
        "        name\n",
        "        location {\n",
        "          coordinates {\n",
        "            latLon {\n",
        "              lat\n",
        "              lon\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# Execute the query. The result is a native python dictionary.\n",
        "data = gql_client.execute(query)\n",
        "trafficRegistrationPoints = data[\"trafficRegistrationPoints\"]\n",
        "\n",
        "# Print the first traffic registration point in the list\n",
        "print(json.dumps(trafficRegistrationPoints[0], indent=4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lshq9h0OR7Ag"
      },
      "source": [
        "Før vi laster opp dataene til BigQuery må vi lage den fulle identifikatoren til den nye tabellen vi skal opprette."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Y2j0sgPPCj0"
      },
      "source": [
        "table_id = \"trafikkregistreringspunkter\"\n",
        "\n",
        "table_path = \".\".join([project_id, dataset_id, table_id])\n",
        "table_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wum-t3N6Sy3o"
      },
      "source": [
        "Dermed er vi klar til å laste opp dataene til BigQuery."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSRNHo-FONj-"
      },
      "source": [
        "bq_client = bigquery.Client(project=project_id)\n",
        "load_job = bq_client.load_table_from_json(data[\"trafficRegistrationPoints\"], table_path)\n",
        "result = load_job.result()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gk3IoZU_TAtF"
      },
      "source": [
        "Du kan nå verifisere at dataene har kommet inn i BigQuery. Hvordan du vil gjøre dette er opp til deg."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qcap7sdZFul"
      },
      "source": [
        "## 3.1) Frivillige oppgaver dersom du har tid til overs\n",
        "\n",
        "**Oppgave 1:** Datastrukturen til `trafficRegistrationPoints` har unødvendig nøsting, i form av at lengde- og breddegrad ligger inn i \"location.coordinates.latLon\". Klarer du å flate ut strukturen i python-kode før dataene skrives til BigQuery? Vi ønsker altså å ende opp med at hvert element bare skal bestå av \"id\", \"name\", \"lat\" og \"lon\". **Hint:** bruk funksjonen `map(fun, iter)` til å mappe om hvert element i lista.\n",
        "\n",
        "**Oppgave 2:** Når vi laster opp data til BigQuery med `load_table_from_json(...)` vil BigQuery forsøke å gjette skjemaet, og alle felter vil bli nullable (i motsetning til required). Det er mulig å angi skjemaet selv. Dette gjør at vi kan sette alle feltene til required, som vi ønsker at de skal være. Gjør dette. **Hint:** Send inn et ekstra argument `job_config=..` til `load_table_from_json(...)`, og sett `schema` i `LoadJobConfig`-objektet du sender inn. [Dokumentasjonen for denne funksjonen kan leses her.](https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.client.Client.html#google.cloud.bigquery.client.Client.load_table_from_json)"
      ]
    }
  ]
}